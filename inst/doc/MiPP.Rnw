% -*- mode: noweb; noweb-default-code-mode: R-mode; -*-
%\VignetteIndexEntry{MiPP Overview}
%\VignetteKeywords{classification}
%\VignetteDepends{MiPP}
%\VignettePackage{MiPP}
%documentclass[12pt, a4paper]{article}
\documentclass[12pt]{article}

\usepackage{amsmath,pstricks}
\usepackage{hyperref}
\usepackage[authoryear,round]{natbib}

\textwidth=6.2in
\textheight=8.5in
%\parskip=.3cm
\oddsidemargin=.1in
\evensidemargin=.1in
\headheight=-.3in

\newcommand{\scscst}{\scriptscriptstyle}
\newcommand{\scst}{\scriptstyle}
\newcommand{\Robject}[1]{{\texttt{#1}}}
\newcommand{\Rfunction}[1]{{\texttt{#1}}}
\newcommand{\Rpackage}[1]{{\textit{#1}}}
\newcommand{\Rfunarg}[1]{{\textit{#1}}}

\newcommand{\acr}{MiPP}
    \newcommand{\word}{Misclassification-Penalized Posteriors}
    \newcommand{\statm}{$\psi_p$}
     \newcommand{\acrs}{MiPP}
    \newcommand{\words}{Misclassification-Penalized Posteriors }
    \newcommand{\statms}{$\psi_p$ }
    \newcommand{\stat}{\psi_p}


\author{Mat Soukup, HyungJun Cho and Jae K. Lee}
\begin{document}
\title{How to use MiPP Package}

\maketitle
\tableofcontents

\section{Introduction}
The \Rpackage{MiPP} package is designed to sequentially add genes to a classification gene model 
based upon the \words (\acr) as discussed in Section 2. The construction of the
model is based upon a training data set and the estimated actual
performance of the model is based upon an independent data set.
When no clear distinction between the training and independent
data sets exists, the cross-validation technique is used. 
For the detailed algorithms, see Soukup, Cho, and Lee (2005) and Soukup and Lee (2004).  
The \Rpackage{MiPP} package employ libraries \Rpackage{MASS} 
for LDA/QDA (linear/quadratic discrimant analysis) and \Rpackage{e1071} for SVM (support vector machine). 
\Rpackage{e1071} should be installed by users.

\section{\words (\acr)}\label{mipp}
In the above section, estimated actual performance is mentioned a
number of times. Classically, the accuracy of a classification is
done by reporting it's estimated actual error rate. However, error
rate fails to take into account how likely a particular sample
belongs to a given class and dichotomizes the data into yes the
sample was correctly classified or no the sample was NOT correctly
classified. Although error rate, plays a key role in how well a
classification model performs it fails to take into account all
the information that is available from a classification rule.

The \words (\acrs) takes into account how likely a sample belongs
to a given class by using a posterior probability of correct
classification. \acrs also adjusts it's definition anytime a
sample is misclassified by subtracting a 1 from the posterior
probability of correct classification resulting in a negative
value of \acrs. If we define the posterior probability of correct
classification using genes $\mathbf{x}$ as $\hat{f}(\mathbf{x})$,
\acrs can be calculated as
\begin{eqnarray}
\psi_p=\sum\limits_{correct}\hat{f}(\mathbf{x})+
\sum\limits_{wrong}\left(\hat{f}(\mathbf{x})-1\right).\label{introSp}
\end{eqnarray}
Here, \emph{correct} refers to the subset of samples that are
correctly classified and \emph{wrong} refers to the subset of
samples that are misclassified. By introducing a random variable
that takes into account whether a sample is misclassified or not
\acrs can be shown to be the sum of posterior probabilities of
correct classification minus the number of misclassified samples.
As a result, \acrs  increases whenever the sum of posterior
probabilities of correction classification increase, the number of
misclassified samples decreases, or both of these occur.

We standardize the MiPP score divided by the number of samples in each data set, 
as denoted sMiPP. Thus, the range of sMiPP is from -1 to 1. 
The more accurate model has the sMiPP score that is closer to 1.

Some basic properties of \acrs are that the maximum value it can
take is equal to the sample size, and on the flip side, the
minimum value is equal to the negation of the sample size. Under a
pure random model, the expected value of \acrs is equal to zero.
The variance is derived and is available from the first author for
the two class case, however an explicit value for more than two
classes can not be derived analytically. Thus, a bootstrapped
estimate is the preferred method of estimating the variance.



\section{Examples}

\subsection{Acute leukemia data:} 
This data has been frequently
used for testing various methods in classification and prediction of
cancer sub-types. Two distinct subsets of array data for AML and ALL
leukemia patients are available: a training set of 27 ALL and 11 AML
samples and a test set of 20 ALL and 14 AML samples. The independent
set was from adult bone marrow samples whereas the independent set
was from 24 bone marrow samples, 10 from peripheral blood samples,
and four of the AML samples from adults. Gene expression levels
contain probes for 6817 human genes from Affymetrix\texttrademark
oligonucleotide microarrays. Note that a sbuset of genes (713 probe sets) 
was stored into the \Rpackage{MiPP} package. 

To run  \Rpackage{MiPP}, the data can be prepared as follows.

\begin{verbatim}
data(leukemia)
     
#IQR normalization     
leukemia <- cbind(leuk1, leuk2)
leukemia <- mipp.preproc(leukemia, data.type="MAS4")
     
#Train set
x.train <- leukemia[,1:38]
y.train <- factor(c(rep("ALL",27),rep("AML",11)),levels=c("ALL","AML"), 
                       labels=c("ALL", "AML"))
#Test set
x.test <- leukemia[,39:72]
y.test <- factor(c(rep("ALL",20),rep("AML",14)),levels=c("ALL","AML"), 
                      labels=c("ALL", "AML"))
\end{verbatim}

Since two distinct data sets exist,  the model is constructed on the training data and evaluated on the test data set 
as follows.

\begin{verbatim}
out <- mipp(x=x.train, y=y.train, x.test=x.test, y.test=y.test,
            nfold=5, percent.cut=0.05, rule="lda")
\end{verbatim}

This sequentially selects genes one gene at a time with the LDA rule and 5-fold cross-validation on the training set.
To reduce computing time, it pre-selects the most plausuable 5\% out of 713 genes by the two-sample t-test, and then performs gene selection.
However, use percent.cut=1 to utilize all genes without pre-selection. The above command generates the following output.

\begin{verbatim}     
out$model

  Order Gene  ErrorRate     MiPP     sMiPP Select
1     1  571 0.11764706 23.91891 0.7034973       
2     2  436 0.02941176 30.41434 0.8945395       
3     3  366 0.02941176 31.35401 0.9221767       
4     4  457 0.02941176 32.14149 0.9453380       
5     5  413 0.02941176 32.17713 0.9463862       
6     6  635 0.00000000 33.75339 0.9927467     **
7     7  648 0.00000000 33.63446 0.9892489       
8     8  181 0.02941176 31.98469 0.9407261   
\end{verbatim}

The gene model with the maximum sMiPP is indicated by two stars (**).
The parsimonious model (indicated by *) is the smallest one of gene models with sMiPPs greater than or equal to 
(max sMiPP - 0.01).  In this example, however, the maximum and parsimonious models (indicated by **) are the same.
Thus, the final model with sMiPP 0.993 contains genes 571, 436, 366, 457, 413, and 635.


\subsection{Colon cancer Data:} 
The colon cancer data set
consists of the 2000 genes with the highest minimal intensity across
the 62 tissue samples out of the original $6,500^+$ genes. The data
set is filtered using the procedures described at their web site.
The 62 samples consist of 40 colon tumor tissue samples and 22
normal colon tissue samples(Alon \emph{et al.}, 1999). Li \emph{et
al.} (2001) identified 5 samples (N34, N36, T30, T33, and T36) which
were likely to have been contaminated. As a result, these five
samples are excluded from any future analysis; our error rate would
be higher if they were included.

Since we are working with a small data set (57 samples), we will be
implementing the procedures described above. The training data set
contains 38 samples (25 tumor and 13 normal samples), and the test
data set contains 19 samples (12 tumor and 7 normal). The choice of
the sizes of the training and independent test set is somewhat
arbitrary, but consistent results were found using a training and
test set of sizes 29 (19 tumor and 10 normal) and 28 (18 tumor and
10 normal), respectively. The colon data set of the \Rpackage{MiPP} package contains only 200 genes 
as an example. For the colon data with no independent test set, \Rpackage{MiPP} can be run as follows.

\begin{verbatim}
data(colon)
x <- mipp.preproc(colon)
y <- colnames(colon)

#Deleting comtaminated chips
x <- x[,-c(51,55,45,49,56)]
y <- y[ -c(51,55,45,49,56)]

out <- mipp(x=x, y=y, nfold=5, p.test=1/3, n.split=20, n.split.eval=100, 
            precent.cut = 0.1 , rule="lda")

\end{verbatim}
This divides the whole data into two groups for training (two-third) and testing (one-third) (p.test = 1/3), 
and  performs the forward gene selection, like for the acute leukemia data.  
Such splitting and selecting are repeated 20 times (n.split=20), generating the following output.

\begin{verbatim}

out$model

    Split Order Gene  ErrorRate      MiPP     sMiPP Select
1       1     1   29 0.05263158 16.032732 0.8438280       
2       1     2  177 0.00000000 18.458082 0.9714780       
3       1     3  163 0.00000000 18.832489 0.9911836     **
4       1     4   36 0.00000000 18.978443 0.9988654      *
5       1     5   51 0.00000000 18.972158 0.9985346       
6       1     6   95 0.00000000 18.969822 0.9984117       
7       2     1   29 0.10526316 14.512517 0.7638167       
8       2     2  102 0.10526316 15.420517 0.8116061       
9       2     3   36 0.05263158 16.652730 0.8764595       
10      2     4  185 0.05263158 16.929696 0.8910366       
11      2     5   76 0.00000000 18.562381 0.9769674     **
12      2     6   78 0.05263158 17.446542 0.9182391       
13      2     7   95 0.05263158 17.138486 0.9020256       
14      3     1   28 0.21052632 10.993642 0.5786127       
15      3     2   36 0.10526316 15.323195 0.8064840       
16      3     3   78 0.00000000 18.692086 0.9837940     **
17      3     4   51 0.05263158 17.047799 0.8972526       
18      3     5   29 0.00000000 18.095243 0.9523812       

.
.
.

128    20     1  163 0.10526316 13.724261 0.7223295       
129    20     2  177 0.00000000 18.774879 0.9881515     **
130    20     3  185 0.00000000 18.825061 0.9907927      *
131    20     4  182 0.05263158 17.033708 0.8965109       
132    20     5   29 0.00000000 18.676012 0.9829480       
\end{verbatim}

For each split, the final model is evaluated by independent 100 splits (n.split.eval=100), generating the following output. 

\begin{verbatim}     
out$model.eval

    Split  G1  G2  G3  G4  G5 G6 G7 mean ErrorRate mean MiPP mean sMiPP
S1      1  29 177 163  NA  NA NA NA   0.0084210526  18.57919  0.9778522
S2      2  29 102  36 185  76 NA NA   0.0173684211  18.26665  0.9614028
S3      3  28  36  78  NA  NA NA NA   0.0005263158  18.74241  0.9864428
S4      4 141 185  49  91 177 36 30   0.0026315789  18.84880  0.9920420
S5      5 163 177  84 185  NA NA NA   0.0010526316  18.70606  0.9845295
S6      6 163 177  36  NA  NA NA NA   0.0000000000  18.74260  0.9864524
S7      7  30  36  78 185  NA NA NA   0.0000000000  18.93579  0.9966204
S8      8  51 185  49  29  36 76 NA   0.0247368421  17.96189  0.9453627
S9      9  30  36  NA  NA  NA NA NA   0.0015789474  18.68832  0.9835957
S10    10  29 177  NA  NA  NA NA NA   0.0110526316  18.28892  0.9625746
S11    11  29 102 163  36  NA NA NA   0.0263157895  17.86323  0.9401701
S12    12  29 177 182  NA  NA NA NA   0.0052631579  18.60552  0.9792380
S13    13  29 177 182  NA  NA NA NA   0.0052631579  18.60552  0.9792380
S14    14  30  36  NA  NA  NA NA NA   0.0015789474  18.68832  0.9835957
S15    15  29 177 185  NA  NA NA NA   0.0042105263  18.76306  0.9875297
S16    16  29 177  36  NA  NA NA NA   0.0063157895  18.66415  0.9823239
S17    17 163 177  NA  NA  NA NA NA   0.0021052632  18.51119  0.9742732
S18    18 163 177  36  NA  NA NA NA   0.0000000000  18.74260  0.9864524
S19    19  28  36 185 177  NA NA NA   0.0000000000  18.91219  0.9953783
S20    20 163 177  NA  NA  NA NA NA   0.0021052632  18.51119  0.9742732

     5% sMiPP 50% sMiPP 95% sMiPP
S1  0.8832269 0.9956378 0.9997555
S2  0.8904381 0.9907046 0.9979650
S3  0.9717611 0.9888683 0.9954501
S4  0.9720076 0.9982314 0.9997744
S5  0.9677334 0.9877863 0.9977993
S6  0.9696978 0.9889706 0.9973368
S7  0.9888911 0.9976407 0.9993538
S8  0.8734358 0.9763289 0.9983271
S9  0.9612196 0.9894887 0.9957796
S10 0.8723262 0.9770533 0.9935208
S11 0.8241824 0.9776791 0.9974065
S12 0.9103882 0.9888216 0.9986135
S13 0.9103882 0.9888216 0.9986135
S14 0.9612196 0.9894887 0.9957796
S15 0.9004550 0.9968640 0.9989926
S16 0.8970961 0.9937537 0.9984018
S17 0.9576879 0.9776923 0.9936058
S18 0.9696978 0.9889706 0.9973368
S19 0.9871570 0.9970437 0.9992126
S20 0.9576879 0.9776923 0.9936058
\end{verbatim}

\vspace{.5in}
{\large \bf Reference}
\begin{itemize}
\item[]     Soukup M., Cho H., and Lee J.K. (2005) Robust classification
     modeling on microarray data  using misclassification penalized
     posterior, {\it Bioinformatics} (forthcoming).
\item[]
     Soukup M. and Lee J.K. (2004) Developing optimal prediction models
     for cancer classification  using gene expression data, {\it Journal of
     Bioinformatics and Computational Biology}, 1(4) 681-694.
\end{itemize}


\end{document}

